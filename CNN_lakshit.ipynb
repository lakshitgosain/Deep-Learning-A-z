{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "introduction to Convolutional Neural Networds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is great to classify some images/ pictures and videos. In this section we are not going to solve any business problem bt will classify the image and determine it's class.\n",
    "\n",
    "We will classify into two categories. To train our model to determine if thre image is a Dog or a cat.\n",
    "We will have a folder full of images if we want to change the categories. E.g- we can repoliavce the pics with the Medicatl Images so that we know if the image has tumor or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course we will need to change the environment we wor upon...as we have to use Images and do image preprocessing instead of .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#We have structureed the Dataset into Test set and Training Set.....We have aslo subdivided the Training Set into Folder For Cats and Dogs separately..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can write some kind of code to extract the label name to specify to the Neural Network whether it is a cat or a dog...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use a better solution using Keras..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have put 4000 images of Cats and Dogs each in the Training Set.We have 1000 of Cats and Dogs each in Test Set...Making a total of 10000 images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will not to do any incoding and splitting it in Training and Test set.\n",
    "#We will need to apply feature scaling....As it is compulsory in Deep learning and Image Processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Libraries and Packages\n",
    "from keras.models import Sequential# Used for having the Neural Network Initiated\n",
    "from keras.layers import Convolution2D # Used for applying the convolutioal Layer\n",
    "from keras.layers import MaxPooling2D# Used for applying MaxPooling\n",
    "from keras.layers import Flatten #used for flattening the Dataset\n",
    "from keras.layers import Dense #Used for adding the Fully connected layer and add the Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the CNN\n",
    "classifier=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LaKgos01\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\", padding=\"same\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Adding the Different Layers\n",
    "classifier.add(Convolution2D(32,3,3,border_mode='same',input_shape=(64,64,3),activation='relu'))\n",
    "\n",
    "#nb_filter= No. of filter= no. of feature Maps that are created..\n",
    "#nb_rows/nb_columns= no. of rows/columns in the Feature Detector matrix\n",
    "#border_mode= THis is to specify how the Feature Detector will handle the borders of the input image\n",
    "#input_shape='shapre of the input image on which the we apply the feature detector through CNN operation'...All our images dont have the same size/Formata and so we need to force them to a fixed size and Format..\n",
    "#remember our images will be converted into 3-d Arrays during the image preprocessing part....each dimension/Channels is a color and it contains the pixels for our images...\n",
    "#image _shape=(3# this is for colored image...We would use for for black and white image\n",
    " #            256 #the number of pixels- dimenrsion of the2D array in each channel\n",
    "  #           256# the number of pixels)\n",
    "\n",
    "#We add the activation fnction= we add this because we need to make sure we dont have any negative pixel values in our feature map....Depending on the parameters we use for the convolution...there can be some negative pixels in feature map in order to have non-linearlity in the problem as classigfication is non-linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pooling- We use the stride of 2\n",
    "#The size of the feature map is divided by 2 after max pooling\n",
    "#We apply max pooling on all of our feature maps. it is necessary to reduce the size of feature maps and therefure reduce the number for nodes in our feature Neural Network\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#pool_size- We will need the size of the Pooled Layer- It is recommended that we use 2*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening\n",
    "\n",
    "#This is crutial step to understand. Put the pools in a single vector.\n",
    "We apply the flattening step and we get a vector that consists of all the different cells of all the feature maps...THis  will later be an input to the future ANN..\n",
    "\n",
    "Q- Why dont we loose all the special structrure by flattening all the feature maps.\n",
    "A- When we create the feature maps , we exctract the special structure by getting the high numbers in the feature maps... hence the high numbers represent the special structures of our images as these high numbers of the feature maps are associated by a specific feature in our image.. in the pooling step we retain these high numbers.\n",
    "\n",
    "Q- Why didn't we take all the pixels of our input image and flatted them into one same single vector wihout applying any previous step.\n",
    "A- This s because if we did not apply the preious steps, then each node of this vector will represent a separate pixel of the image and does not provide any information around the pixel....so we only get information of the pixel itself and not get the information of how these pixels are spatially connectd with the pixels around it.\n",
    "#However if we apply the stpes properly, then since each feature map corresponds to a specific feature of the image(by that High Number), this is because the high number does not represent a pixel itself and indeed provieds infor about a feature of the image..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Full Connect Step\n",
    "\n",
    "It sonsists of creating a classign ANN composed of soe specific layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manage to cponvert our invput image to 1-d vector. We will now use this vector as input layer of our ANN...It can be great classifier of non-linear problems.\n",
    "\n",
    "We need to now create hidden layer as this is called the fully connected layer as well as a Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LaKgos01\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\LaKgos01\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim=128,activation='relu'))#thi sis the Hidden layer\n",
    "\n",
    "#number of nodes in the hidden layer.. choice is random...just make sure it's neither too small nor too large..\n",
    "\n",
    "classifier.add(Dense(output_dim=1,activation='sigmoid'))# this is the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compile the whole model..by using schatostic gradient descent, Loss Function and a performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now fit the CNN that we have built to all the images.\n",
    "\n",
    "We will do it in one step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras Documentation--image augmentation...it exists to preprocess imgaes to prenvent over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Image Augmentation Process. in the browser type keras documentation.\n",
    "Over-fitting happens when the data is fitted over a few observations but fails to generalize these observations to the new Dataset...We need a lot of images to find corellations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working with 10000 images which is not much....we can use a trick..this is where daat augmentation comes into play...several batched of our images wil be created and in each bacthc random transformations will be performed...they wil be flipped, rotated etc.Hence all the batches will use different version of the same images.\n",
    "\n",
    "Hence, image augmentation will helps us enrich out training set even with the small amount of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are using ready to use code form Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2437s 305ms/step - loss: 0.4160 - accuracy: 0.8025 - val_loss: 0.4909 - val_accuracy: 0.7537\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 2270s 284ms/step - loss: 0.1967 - accuracy: 0.9206 - val_loss: 0.4832 - val_accuracy: 0.7575\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 2382s 298ms/step - loss: 0.1128 - accuracy: 0.9576 - val_loss: 5.3928 - val_accuracy: 0.7608\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 2442s 305ms/step - loss: 0.0783 - accuracy: 0.9711 - val_loss: 1.9252 - val_accuracy: 0.7561\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 2304s 288ms/step - loss: 0.0598 - accuracy: 0.9790 - val_loss: 4.8508 - val_accuracy: 0.7540\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 2245s 281ms/step - loss: 0.0485 - accuracy: 0.9832 - val_loss: 1.5486 - val_accuracy: 0.7524\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 30579s 4s/step - loss: 0.0419 - accuracy: 0.9860 - val_loss: 0.7646 - val_accuracy: 0.7407\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 4519s 565ms/step - loss: 0.0360 - accuracy: 0.9880 - val_loss: 1.5351 - val_accuracy: 0.7468\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 2350s 294ms/step - loss: 0.0327 - accuracy: 0.9895 - val_loss: 1.6450 - val_accuracy: 0.7567\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 2532s 316ms/step - loss: 0.0285 - accuracy: 0.9905 - val_loss: 1.0205 - val_accuracy: 0.7564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1aa1b53b208>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(#image augmentation of Training set\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)#image augmentation for test set\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(64, 64),#as used previously above,\n",
    "        batch_size=32,#no of images wafter which the weighs are updated,\n",
    "        class_mode='binary')#as we have bnary outcome\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "classifier.fit(\n",
    "        training_set,\n",
    "        steps_per_epoch=8000,#no. of images in training set,\n",
    "        epochs=10,\n",
    "        validation_data=test_set,\n",
    "        validation_steps=2000)#images in out test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Con"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
